{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Machine Learning Model Deployment and Management: A Hands-on Tutorial using MLflow</b><br>\n",
    "MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It provides tools for tracking experiments, packaging code into reproducible runs, and deploying models. In this tutorial, we will explore the basic functionalities of MLflow for experiment tracking.\n",
    "\n",
    "**PLEASE READ THE README.md BEFORE RUNNING THIS NOTEBOOK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports**\n",
    "\n",
    "Main packages used in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a demo dataset from sklearn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# MLFlow related imports, loads the main module and the package for sklearn\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# For finding the best model\n",
    "from mlflow.tracking.client import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "# Pandas and matplotlib are used for data manipulation and data visualization respectively\n",
    "import numpy as np\n",
    "\n",
    "# ML related imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Model imports\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# For type annotations\n",
    "from sklearn.base import BaseEstimator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Starting the MLflow UI** \n",
    "\n",
    "**Manual Process: You must complete this step manually.**\n",
    "\n",
    "If you are on a Windows PC:\n",
    "- And if you installed Anaconda, start the Anaconda Command Prompt, then **browse to the folder that contains this Jupyter notebook** and execute the following code: `mlflow ui`\n",
    "- If you did not install Anaconda but you are running Python on your own, then start a terminal window, then **browse to the folder that contains this Jupyter notebook** and execute the following: `python -m mlflow ui`\n",
    "\n",
    "If you are on a MAC: start your terminal, then **browse to the folder that contains this Jupyter notebook** and execute the following code: `mlflow ui`\n",
    "\n",
    "When this command is run in the active terminal window, **it will freeze the session** and will not stop until a stop (Ctrl+C) command is executed (or the terminal window is closed). **DO NOT CLOSE THIS TERMINAL WINDOW UNTIL THE END OF THE TUTORIAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual Process: To access the MLflow UI, open a browser window and go to the following link:** http://localhost:5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **0. Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Data from sklearn Datasets\n",
    "\n",
    "The dataset chosen is the California Hoursing price dataset, provided in StatLib\n",
    "\n",
    "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
    "\n",
    "#### Data Set Characteristics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fetch_california_housing()[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the housing dataset\n",
    "california_housing_df = fetch_california_housing()\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = california_housing_df.data\n",
    "y = california_housing_df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limit the size of the dataset for faster execution\n",
    "# Ranodmly select 2000 samples from the dataset\n",
    "np.random.seed(42)\n",
    "indices = np.random.choice(X.shape[0], size=2000, replace=False)\n",
    "\n",
    "X_cut = X[indices]\n",
    "y_cut = y[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing a train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_cut, y_cut, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Logging experiments**\n",
    "\n",
    "There are several ways to log and compare runs in mlflow. This tutorial will cover the most common options with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1.1 Creating an experiment**\n",
    "\n",
    "_Experiments_ in MLflow allow for efficient comparison between runs.\n",
    "\n",
    "Use **_create_experiment()_** to define a new experiment with a name and as many tags as needed. This can also be done directly from the UI.\n",
    "\n",
    "_Experiments_ are defined using **_mlflow.set_experiment()_**, and passing a string with the name of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the experiment name\n",
    "experiment_name = \"Example Experiment\" # It can contain whitespaces or special characters, but it will make code commands harder to perform\n",
    "\n",
    "# Provide an Experiment description that will appear in the UI\n",
    "experiment_description = (\n",
    "\"This is the example of a tag, in this case the description.\"\n",
    ")\n",
    "experiment_tags = {\n",
    "    \"mlflow.note.content\": experiment_description,\n",
    "}\n",
    "\n",
    "# Check if the experiment already exists, if not, create it\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "if experiment is None:\n",
    "    mlflow.create_experiment(\n",
    "    name=experiment_name, tags=experiment_tags\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.2 Automatic Logging**\n",
    "\n",
    "By default, MLflow allows logging metrics, parameters, and models without the need for explicit log statements. All that is needed is to call the appropriate `autolog()` method before the training code. \n",
    "\n",
    "For some libraries it **only logs the train metrics**. Make sure to check the documantation to avoid logging the wrong metrics:\n",
    "\n",
    "https://mlflow.org/docs/latest/tracking/autolog.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the experiment to the one that was just created\n",
    "mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "# Enable automatic logging of scikit-learn models with MLflow\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# Create an instance of the model\n",
    "rfr = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Fit the train data, this will be automatically logged with autolog, including TRAIN metrics\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "mlflow.end_run() # This lets MLflow know it must not log anything else from this point forward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of this run can be seen in the UI:\n",
    "\n",
    "http://localhost:5000\n",
    "\n",
    "Users can click on the experiment name, then click on the model run. On the model's \"Overview\" page, the model's parameters can be observed. Clicking on the \"Model metrics\" tab shows the training metrics captured for the model. \n",
    "\n",
    "Alternatively, users can bring model runs back to this notebook and organize it using a Pandas DataFrame (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_experiment = dict(mlflow.get_experiment_by_name(\"Example Experiment\"))\n",
    "\n",
    "experiment_id = current_experiment[\"experiment_id\"] # Extract the experiment ID\n",
    "\n",
    "search_result_df = mlflow.search_runs(\n",
    "    experiment_ids=experiment_id, # Define the scope of the search\n",
    "    run_view_type=ViewType.ALL, # Select the type of run\n",
    "    output_format=\"pandas\", # Select the output format\n",
    ")\n",
    "\n",
    "search_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.3 Manual Logging**\n",
    "\n",
    "In most situations, it will be neccessary to track metrics that are not registered by default. In this case, the MLflow API has an extensive set of methods to customize what is logged (see https://mlflow.org/docs/latest/tracking/tracking-api.html#tracking-logging-functions)\n",
    "\n",
    "Some of the basic functions are:\n",
    "\n",
    "**_mlflow.start_run()_**: Starts the run and designates it as the current run for logging metrics and parameters.\n",
    "\n",
    "**_mlflow.log_metric()_**: Logs an specific custom metric. It is stored in a single key-value pair, where the key is the metric name and the value the actual value of the metric.\n",
    "\n",
    "**_mlflow.end_run()_**: As seen before, ends the current run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new experiment\n",
    "\n",
    "While we could still use our Example Experiment, it wouldn't provide much in the sense of comparison, since the sklearn.autolog method only registers TRAIN metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To begin, we will create a separate experiment for custom logging.\n",
    "new_experiment_name = \"California Housing\" \n",
    "\n",
    "# Provide an Experiment description that will appear in the UI\n",
    "new_experiment_description = (\n",
    "\"This experiment tests the performance of a regression model on the California Housing dataset.\"\n",
    ")\n",
    "new_experiment_tags = {\n",
    "    \"mlflow.note.content\": new_experiment_description,\n",
    "}\n",
    "\n",
    "# Check if the experiment already exists, if not, create it\n",
    "experiment = mlflow.get_experiment_by_name(new_experiment_name)\n",
    "if experiment is None:\n",
    "    mlflow.create_experiment(\n",
    "    name=new_experiment_name, tags=new_experiment_tags\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually log \"mean squared error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required methods for calculating metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Set the experiment to the one that was just created\n",
    "mlflow.set_experiment(experiment_name=new_experiment_name)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = RandomForestRegressor(n_estimators=1, max_depth=1, max_features=1, random_state=42)\n",
    "\n",
    "# Create a custom name for the run\n",
    "test_run_name = \"Manual_Run_Test\"\n",
    "\n",
    "# We must keep the run open, so MLflow knows where to track, we use the with function for that\n",
    "with mlflow.start_run(run_name=test_run_name):\n",
    "        # Train the model, and calculate any variables required for the metrics\n",
    "        model.fit(X_train, y_train)\n",
    "        train_y_pred = model.predict(X_train)\n",
    "        test_y_pred = model.predict(X_test)\n",
    "        # Calculate and log the metrics\n",
    "        train_mse = mean_squared_error(y_train, train_y_pred)\n",
    "        mlflow.log_metric(\"train_mse\", train_mse)\n",
    "        test_mse = mean_squared_error(y_test, test_y_pred)\n",
    "        mlflow.log_metric(\"test_mse\", test_mse)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this custom run can also be seen in the UI or by using the search_run method with the new experiment ID (as shown below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_experiment = dict(mlflow.get_experiment_by_name(new_experiment_name))\n",
    "\n",
    "experiment_id = current_experiment[\"experiment_id\"] # Extract the experiment ID\n",
    "\n",
    "search_result_df = mlflow.search_runs(\n",
    "    experiment_ids=experiment_id, # Define the scope of the search\n",
    "    run_view_type=ViewType.ALL, # Select the type of run\n",
    "    output_format=\"pandas\", # Select the output format\n",
    ")\n",
    "\n",
    "search_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Function to Capture Manual Logs\n",
    "\n",
    "The process of logging this metrics and performing this runs can be encapsulated in a function, such as the one shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_experiment_run(\n",
    "    model: BaseEstimator,\n",
    "    run_name: str,\n",
    "    X_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    y_test: np.ndarray\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Train a machine learning model, evaluate its accuracy on both training and testing data, log these\n",
    "    performance metrics using MLflow, and print the results. Additionally, it registers the model in MLflow with the given run name.\n",
    "\n",
    "    This function takes a scikit-learn compatible model, fits it on the provided training data, evaluates it on both the training and testing datasets,\n",
    "    logs the accuracy metrics using MLflow, prints the train and test accuracy, and finally registers the model in MLflow using the provided run name and model description.\n",
    "\n",
    "    Parameters:\n",
    "    - model: BaseEstimator\n",
    "        An instance of a scikit-learn compatible estimator (model) to be trained and evaluated.\n",
    "    - run_name: str\n",
    "        A string representing the name of the MLflow run, which will also be used as the model name when registering in MLflow.\n",
    "    - X_train: np.ndarray\n",
    "        A numpy array containing the features of the training data.\n",
    "    - X_test: np.ndarray\n",
    "        A numpy array containing the features of the test data.\n",
    "    - y_train: np.ndarray\n",
    "        A numpy array containing the true values of the training data.\n",
    "    - y_test: np.ndarray\n",
    "        A numpy array containing the true values of the test data.\n",
    "\n",
    "\n",
    "    Returns:\n",
    "    - run_id: The ID of the model run, in case the model is to be registered\n",
    "    \"\"\"\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        model.fit(X_train, y_train)\n",
    "        train_y_pred = model.predict(X_train)\n",
    "        test_y_pred = model.predict(X_test)\n",
    "        train_mse = mean_squared_error(y_train, train_y_pred)\n",
    "        mlflow.log_metric(\"train_mse\", train_mse)\n",
    "        test_mse = mean_squared_error(y_test, test_y_pred)\n",
    "        mlflow.log_metric(\"test_mse\", test_mse)\n",
    "        run_id = mlflow.active_run().info.run_uuid\n",
    "        print(f\"Model run: {run_id}\")\n",
    "\n",
    "    # Inform MLFlow that the run is over\n",
    "    mlflow.end_run()\n",
    "    return run_id # The ID is used to programatically register a model, while it can be extracted from the search_runs method, it's much simpler to just return it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use it, just replace the main code with the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the experiment to the one that was just created\n",
    "mlflow.set_experiment(experiment_name=new_experiment_name)\n",
    "\n",
    "# Create an instance of the model\n",
    "rfr = RandomForestRegressor(n_estimators=1, max_depth=2, max_features=2, random_state=42)\n",
    "\n",
    "# Create a custom name for the run\n",
    "run_name = \"RFR_California\"\n",
    "\n",
    "model_run_id = perform_experiment_run(model=rfr, run_name=run_name, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Registering a model**\n",
    "Once a model with the desired performance is found, it can be registered in MLFlow for future use. While this can be done during an MLFlow run using the mlflow.log_model method, if the model is to be registered after logging, the ID of that particular model run needs to be passed as a part of the URI to locate the model, as shown in the next line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = f\"runs:/{model_run_id}/model\"\n",
    "model_name = \"california-random-forest-r\" # It's important to give the model a name that is shell appropiate (no spaces, no reserved characters)\n",
    "mlflow.register_model(model_uri, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, a model can be manually promoted from the UI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Transitioning models between stages**\n",
    "\n",
    "Once a good run with a specific model is identified, it is common to promote the model from a developement phase to several stages all the way up to Production. While traditionally models would be moved between stages, in more recent versions of MLflow, this has been substituted by using aliases. The most common convention is to call the best model Champion and any possible upgrades as Challenger. To to do so, the following method can be used:\n",
    "\n",
    "**_{client}.set_registered_model_alias()_**\n",
    "\n",
    "Where the main parameters are the name of the run, the alias, and the version of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a model the Champion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient() # Create an instance of the client, that allows to programmatically perform the same operations as in the UI\n",
    "client.set_registered_model_alias(model_name, \"champion\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 Building Alternative Models**\n",
    "\n",
    "While the Random Forest model performs well and with great accuracy, some hyperparameter tuning might yield an event better one. \n",
    "\n",
    "To this end, usually a Grid Search Cross Validation or other form of comparison is performed between some of the most common parameters. There is one problem about it with MLflow, and it is that the default GridSearchCV class does not allow the use of custom metrics, and so **it would only work with autolog(), resulting in logging of only TRAIN metrics**. This is why we disable autologging, find the best model, then record its custom metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parameter grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A basic Grid Search Cross Validation is enough to find an improved model from the base one\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the hyperparameters and their possible values\n",
    "param_grid = {\n",
    "    \"n_estimators\": [25, 50, 100, 150],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"max_depth\": [3, 6, 9],\n",
    "    \"max_leaf_nodes\": [3, 6, 9],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temporarily disable autologging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary disable autologging\n",
    "mlflow.sklearn.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search will perform runs with every possible combination\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), param_grid=param_grid)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable autologging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autologging\n",
    "mlflow.sklearn.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best model from the grid search\n",
    "best_gscv_model = grid_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters for the model\n",
    "print(best_gscv_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to log custom metrics, the best model must be run manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the experiment to the one that was just created\n",
    "mlflow.set_experiment(experiment_name=new_experiment_name)\n",
    "\n",
    "# Create a custom name for the run\n",
    "run_name = \"Best_GSCV_Run\"\n",
    "\n",
    "model_run_id = perform_experiment_run(model=best_gscv_model, run_name=run_name, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Identifying the best run**\n",
    "\n",
    "After all the runs, the best model can be identified by defining what filters and conditions would make it qualify as such, usually just ordering by the metric (or metrics) of interest is enough.\n",
    "\n",
    "Before that, and for ease of use and compatibility, it's best to switch from working with the experiment name, and rely on the experiment ID. This is easy to achieve, as shown in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_experiment = dict(mlflow.get_experiment_by_name(new_experiment_name))\n",
    "\n",
    "experiment_id = current_experiment[\"experiment_id\"] # Extract the experiment ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_run = mlflow.search_runs(\n",
    "    experiment_ids=experiment_id, # Define the scope of the search\n",
    "    filter_string=\"\", # Add specific filtering conditions\n",
    "    run_view_type=ViewType.ALL, # Select the type of run\n",
    "    max_results=1, # Filter the number of results desired, in this case since it's the best model, only one is required\n",
    "    order_by=[\"metrics.test_mse ASC\"], # Specify an order by condition\n",
    "    output_format=\"pandas\", # Select the output format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_run_id = best_model_run[\"run_id\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_run_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3 Registering the best run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_uri = f\"runs:/{best_model_run_id}/model\"\n",
    "mlflow.register_model(best_model_uri, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then alias this new version as a Challenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.set_registered_model_alias(model_name, \"challenger\",2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Using registered models**\n",
    "\n",
    "Once a model is registered in MLFlow, it can be easily extracted for use in other projects via the URI, and either the version or the alias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "\n",
    "champion_model = mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}@champion\") #Using the alias\n",
    "challenger_model = mlflow.pyfunc.load_model(model_uri=f\"models:/{model_name}@challenger\") #Using the alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random number to select a random observation from the test set\n",
    "random_number = np.random.randint(0, len(X_test))\n",
    "\n",
    "# Select the random observation from the test set\n",
    "random_observation = X_test[random_number].reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "champion_model.predict(random_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "challenger_model.predict(random_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[random_number]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **5. Model Serving**\n",
    "\n",
    "Registered models can be served using the MLFlow API. To do so, complete the below steps:\n",
    "\n",
    "**Manual process:**\n",
    "\n",
    "**1) If you are on Windows: start a new Anaconda PowerShell Prompt window. If you are on a MAC: start a new terminal window.**\n",
    "\n",
    "**2) Navigate to the \"MLflow Tutorial\" folder you created for this tutorial**\n",
    "\n",
    "In this window, execute the following code:\n",
    "\n",
    "`mlflow models serve --model-uri models:/california-random-forest-r@champion -p 1234 --no-conda`\n",
    "\n",
    "Please note that, the template the serve models is as follows:\n",
    "\n",
    "`mlflow models serve --model-uri models:/{model_name}@{alias} -p 1234 --no-conda # If using the alias convention`\n",
    "\n",
    "**Here is where the alias method shines the most. By using aliases, the serve command remains the same.**\n",
    "\n",
    "**Finding the model name**: The model name can be extracted from the UI, or from the query performed before. In the MLflow UI, click on \"Models\" on the upper left corner and select the desired model. After that just fill the {model_name} with it and use \"champion\" for {alias} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.1 Using the model API**\n",
    "\n",
    "Once a model has been served in a specific port, performing an API call on it with some data to predict will return the predictions from that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define some test data in JSON format to pass to the endpoint to make a prediction\n",
    "data = {\"inputs\": X_test[:3].tolist()}\n",
    "\n",
    "# Convert the data dictionary to JSON string\n",
    "payload = json.dumps(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "host = \"127.0.0.1\"\n",
    "port = \"1234\"\n",
    "\n",
    "url = f\"http://{host}:{port}/invocations\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# Perform a POST request to the local hosted MLFlow server using the port defined while serving the model\n",
    "response = requests.post(url=url, headers=headers, data=payload)\n",
    "\n",
    "print(f\"Predictions: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5.2 Updating a Champion**\n",
    "\n",
    "The champion and challenger alias convention is used to easily serve models without having to modify the version of the current model, as explained above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.set_registered_model_alias(model_name, \"champion\",2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with just re-starting the served model, it will use the second version of the model as the one to serve. \n",
    "\n",
    "**End the current serving process (Ctrl+C or Cmd+C) then click then re-run the same code as before. Clicking arrow Up should bring the last command to the terminal.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_response = requests.post(url=url, headers=headers, data=payload)\n",
    "print(f\"Predictions: {new_response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Initial Champion prediction: {response.text} \\n Challeger (New Champion) prediction: {new_response.text} \\n Real Value: {y_test[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NOTE:**\n",
    "**REMEMBER TO CLOSE ALL TERMINALS AND DELETE THE \"mlruns\" FOLDER BEFORE RUNNING AGAIN**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
